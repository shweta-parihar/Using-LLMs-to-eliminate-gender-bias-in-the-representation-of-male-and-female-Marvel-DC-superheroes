{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69cdbd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, running on CPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"GPU not available, running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b9a144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, n_vocab=12000, epochs=121, n_hidden_G=512, n_layers_G=2, n_hidden_E=512, n_layers_E=1, n_z=100, word_dropout=0.5, rec_coef=7, lr=0.0001, gpu_device=1, n_highway_layers=2, n_embed=300, unk_token='<unk>', pad_token='<pad>', start_token='<sos>', end_token='<eos>', resume_training=False, to_train=True)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 303\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m opt\u001b[38;5;241m.\u001b[39mto_train:\n\u001b[0;32m--> 303\u001b[0m         training()\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         generate_sentences(\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 247\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m train_rec_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    246\u001b[0m train_kl_loss \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_iter:\n\u001b[1;32m    248\u001b[0m     x \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    249\u001b[0m     G_inp \u001b[38;5;241m=\u001b[39m create_generator_input(x, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/data/iterator.py:177\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m             minibatch\u001b[38;5;241m.\u001b[39msort(key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort_key, reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m Batch(minibatch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    178\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepeat:\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/data/batch.py:22\u001b[0m, in \u001b[0;36mBatch.__init__\u001b[0;34m(self, data, dataset, device, train)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m field \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     batch \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[name] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, field\u001b[38;5;241m.\u001b[39mprocess(batch, device\u001b[38;5;241m=\u001b[39mdevice, train\u001b[38;5;241m=\u001b[39mtrain))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/data/field.py:184\u001b[0m, in \u001b[0;36mField.process\u001b[0;34m(self, batch, device, train)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Process a list of examples to create a torch.Tensor.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03mPad, numericalize, and postprocess a batch and create a tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m        and custom postprocessing Pipeline.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m padded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad(batch)\n\u001b[0;32m--> 184\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumericalize(padded, device\u001b[38;5;241m=\u001b[39mdevice, train\u001b[38;5;241m=\u001b[39mtrain)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torchtext/data/field.py:305\u001b[0m, in \u001b[0;36mField.numericalize\u001b[0;34m(self, arr, device, train)\u001b[0m\n\u001b[1;32m    303\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 305\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mcuda(device)\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minclude_lengths:\n\u001b[1;32m    307\u001b[0m         lengths \u001b[38;5;241m=\u001b[39m lengths\u001b[38;5;241m.\u001b[39mcuda(device)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:239\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "my_punc = \"!\\\"#$%&\\()*+?_/:;[]{}|~,`\"\n",
    "table = dict((ord(char), u' ') for char in my_punc)\n",
    "\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"\\'s \", \" \", string)\n",
    "    string = re.sub(r\"\\'m \", \" \", string)\n",
    "    string = re.sub(r\"\\'ve \", \" \", string)\n",
    "    string = re.sub(r\"n\\'t \", \" not \", string)\n",
    "    string = re.sub(r\"\\'re \", \" \", string)\n",
    "    string = re.sub(r\"\\'d \", \" \", string)\n",
    "    string = re.sub(r\"\\'ll \", \" \", string)\n",
    "    string = re.sub(\"-\", \" \", string)\n",
    "    string = re.sub(r\"@\", \" \", string)\n",
    "    string = re.sub('\\'', '', string)\n",
    "    string = string.translate(table)\n",
    "    string = string.replace(\"..\", \"\").strip()\n",
    "    return string\n",
    "\n",
    "def tokenizer_function(text):\n",
    "    text = [x for x in text.split(\" \") if x != \"\" and x.find(\" \") == -1]\n",
    "    return text\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, path, text_field, **kwargs):\n",
    "        fields = [('text', text_field)]\n",
    "        examples = []\n",
    "        with open(path, 'r') as f:\n",
    "            for text in f:\n",
    "                examples.append(data.Example.fromlist([text], fields))\n",
    "        super(MyDataset, self).__init__(examples, fields, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, text_field, train='train', **kwargs):\n",
    "        return super(MyDataset, cls).splits(text_field=text_field, train=train, **kwargs)\n",
    "\n",
    "def get_iterators(opt):\n",
    "    text_field = data.Field(init_token=opt.start_token, eos_token=opt.end_token, lower=True, tokenize=tokenizer_function, batch_first=True)\n",
    "    train_data, val_data = MyDataset.splits(path=\"\", train=\"train.txt\", test=\"test.txt\", text_field=text_field)\n",
    "    text_field.build_vocab(train_data, val_data, max_size=opt.n_vocab-4, vectors='glove.6B.300d')\n",
    "    train_vocab = text_field.vocab\n",
    "\n",
    "    train_iter, val_iter = data.BucketIterator.splits((train_data, val_data), batch_size=opt.batch_size, sort_key=lambda x: len(x.text), repeat=False)\n",
    "    return train_iter, val_iter, train_vocab\n",
    "\n",
    "def get_cuda(tensor):\n",
    "    return tensor\n",
    "\n",
    "def get_sentences_in_batch(x, vocab):\n",
    "    for sent in x:\n",
    "        str1 = \"\"\n",
    "        for word in sent:\n",
    "            str1 += vocab.itos[word] + \" \"\n",
    "        print(str1)\n",
    "\n",
    "class Highway(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Highway, self).__init__()\n",
    "        self.n_layers = opt.n_highway_layers\n",
    "        self.non_linear = nn.ModuleList([nn.Linear(opt.n_embed, opt.n_embed) for _ in range(self.n_layers)])\n",
    "        self.linear = nn.ModuleList([nn.Linear(opt.n_embed, opt.n_embed) for _ in range(self.n_layers)])\n",
    "        self.gate = nn.ModuleList([nn.Linear(opt.n_embed, opt.n_embed) for _ in range(self.n_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in range(self.n_layers):\n",
    "            gate = torch.sigmoid(self.gate[layer](x))\n",
    "            non_linear = F.relu(self.non_linear[layer](x))\n",
    "            linear = self.linear[layer](x)\n",
    "            x = gate * non_linear + (1 - gate) * linear\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.highway = Highway(opt)\n",
    "        self.n_hidden_E = opt.n_hidden_E\n",
    "        self.n_layers_E = opt.n_layers_E\n",
    "        self.lstm = nn.LSTM(input_size=opt.n_embed, hidden_size=opt.n_hidden_E, num_layers=opt.n_layers_E, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0 = torch.zeros(2*self.n_layers_E, batch_size, self.n_hidden_E)\n",
    "        c_0 = torch.zeros(2*self.n_layers_E, batch_size, self.n_hidden_E)\n",
    "        self.hidden = (get_cuda(h_0), get_cuda(c_0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, n_seq, n_embed = x.size()\n",
    "        x = self.highway(x)\n",
    "        self.init_hidden(batch_size)\n",
    "        _, (self.hidden, _) = self.lstm(x, self.hidden)\n",
    "        self.hidden = self.hidden.view(self.n_layers_E, 2, batch_size, self.n_hidden_E)\n",
    "        self.hidden = self.hidden[-1]\n",
    "        e_hidden = torch.cat(list(self.hidden), dim=1)\n",
    "        return e_hidden\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Generator, self).__init__()\n",
    "        self.n_hidden_G = opt.n_hidden_G\n",
    "        self.n_layers_G = opt.n_layers_G\n",
    "        self.n_z = opt.n_z\n",
    "        self.lstm = nn.LSTM(input_size=opt.n_embed+opt.n_z, hidden_size=opt.n_hidden_G, num_layers=opt.n_layers_G, batch_first=True)\n",
    "        self.fc = nn.Linear(opt.n_hidden_G, opt.n_vocab)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0 = torch.zeros(self.n_layers_G, batch_size, self.n_hidden_G)\n",
    "        c_0 = torch.zeros(self.n_layers_G, batch_size, self.n_hidden_G)\n",
    "        self.hidden = (get_cuda(h_0), get_cuda(c_0))\n",
    "\n",
    "    def forward(self, x, z, g_hidden=None):\n",
    "        batch_size, n_seq, n_embed = x.size()\n",
    "        z = torch.cat([z]*n_seq, 1).view(batch_size, n_seq, self.n_z)\n",
    "        x = torch.cat([x, z], dim=2)\n",
    "\n",
    "        if g_hidden is None:\n",
    "            self.init_hidden(batch_size)\n",
    "        else:\n",
    "            self.hidden = g_hidden\n",
    "\n",
    "        output, self.hidden = self.lstm(x, self.hidden)\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return output, self.hidden\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding = nn.Embedding(opt.n_vocab, opt.n_embed)\n",
    "        self.encoder = Encoder(opt)\n",
    "        self.hidden_to_mu = nn.Linear(2*opt.n_hidden_E, opt.n_z)\n",
    "        self.hidden_to_logvar = nn.Linear(2*opt.n_hidden_G, opt.n_z)\n",
    "        self.generator = Generator(opt)\n",
    "        self.n_z = opt.n_z\n",
    "\n",
    "    def forward(self, x, G_inp, z=None, G_hidden=None):\n",
    "        if z is None:\n",
    "            batch_size, n_seq = x.size()\n",
    "            x = self.embedding(x)\n",
    "            E_hidden = self.encoder(x)\n",
    "            mu = self.hidden_to_mu(E_hidden)\n",
    "            logvar = self.hidden_to_logvar(E_hidden)\n",
    "            z = torch.randn([batch_size, self.n_z])\n",
    "            z = mu + z * torch.exp(0.5 * logvar)\n",
    "            kld = -0.5 * torch.sum(logvar - mu.pow(2) - logvar.exp() + 1, 1).mean()\n",
    "        else:\n",
    "            kld = None\n",
    "\n",
    "        G_inp = self.embedding(G_inp)\n",
    "\n",
    "        logit, G_hidden = self.generator(G_inp, z, G_hidden)\n",
    "        return logit, G_hidden, kld\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--n_vocab', type=int, default=12000)\n",
    "parser.add_argument('--epochs', type=int, default=121)\n",
    "parser.add_argument('--n_hidden_G', type=int, default=512)\n",
    "parser.add_argument('--n_layers_G', type=int, default=2)\n",
    "parser.add_argument('--n_hidden_E', type=int, default=512)\n",
    "parser.add_argument('--n_layers_E', type=int, default=1)\n",
    "parser.add_argument('--n_z', type=int, default=100)\n",
    "parser.add_argument('--word_dropout', type=float, default=0.5)\n",
    "parser.add_argument('--rec_coef', type=float, default=7)\n",
    "parser.add_argument('--lr', type=float, default=0.0001)\n",
    "parser.add_argument('--gpu_device', type=int, default=1)\n",
    "parser.add_argument('--n_highway_layers', type=int, default=2)\n",
    "parser.add_argument('--n_embed', type=int, default=300)\n",
    "parser.add_argument('--unk_token', type=str, default=\"<unk>\")\n",
    "parser.add_argument('--pad_token', type=str, default=\"<pad>\")\n",
    "parser.add_argument('--start_token', type=str, default=\"<sos>\")\n",
    "parser.add_argument('--end_token', type=str, default=\"<eos>\")\n",
    "\n",
    "def str2bool(v):\n",
    "    if v.lower() == 'true':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "parser.add_argument('--resume_training', type=bool, default=False)\n",
    "parser.add_argument('--to_train', type=bool, default=True)\n",
    "\n",
    "\n",
    "opt, unknown = parser.parse_known_args()\n",
    "print(opt)\n",
    "save_path = \"saved_models/vae_model.tar\"\n",
    "if not os.path.exists(\"saved_models\"):\n",
    "    os.makedirs(\"saved_models\")\n",
    "\n",
    "train_iter, val_iter, vocab = get_iterators(opt)\n",
    "\n",
    "vae = VAE(opt)\n",
    "vae.embedding.weight.data.copy_(vocab.vectors)\n",
    "vae = get_cuda(vae)\n",
    "trainer_vae = torch.optim.Adam(vae.parameters(), lr=opt.lr)\n",
    "\n",
    "def create_generator_input(x, train):\n",
    "    G_inp = x[:, 0:x.size(1)-1].clone()\n",
    "    if train is False:\n",
    "        return G_inp\n",
    "\n",
    "    r = np.random.rand(G_inp.size(0), G_inp.size(1))\n",
    "    for i in range(len(G_inp)):\n",
    "        for j in range(1, G_inp.size(1)):\n",
    "            if r[i, j] < opt.word_dropout and G_inp[i, j] not in [vocab.stoi[opt.pad_token], vocab.stoi[opt.end_token]]:\n",
    "                G_inp[i, j] = vocab.stoi[opt.unk_token]\n",
    "\n",
    "    return G_inp\n",
    "\n",
    "def train_batch(x, G_inp, step, train=True):\n",
    "    logit, _, kld = vae(x, G_inp, None, None)\n",
    "    logit = logit.view(-1, opt.n_vocab)\n",
    "    x = x[:, 1:x.size(1)]\n",
    "    x = x.contiguous().view(-1)\n",
    "    rec_loss = F.cross_entropy(logit, x)\n",
    "    kld_coef = (math.tanh((step - 15000)/1000) + 1) / 2\n",
    "    loss = opt.rec_coef * rec_loss + kld_coef * kld\n",
    "    if train is True:\n",
    "        trainer_vae.zero_grad()\n",
    "        loss.backward()\n",
    "        trainer_vae.step()\n",
    "    return rec_loss.item(), kld.item()\n",
    "\n",
    "def load_model_from_checkpoint():\n",
    "    global vae, trainer_vae\n",
    "    checkpoint = torch.load(save_path, map_location='cpu')\n",
    "    vae.load_state_dict(checkpoint['vae_dict'])\n",
    "    trainer_vae.load_state_dict(checkpoint['vae_trainer'])\n",
    "    return checkpoint['step'], checkpoint['epoch']\n",
    "\n",
    "def training():\n",
    "    start_epoch = step = 0\n",
    "    if opt.resume_training:\n",
    "        step, start_epoch = load_model_from_checkpoint()\n",
    "    for epoch in range(start_epoch, opt.epochs):\n",
    "        vae.train()\n",
    "        train_rec_loss = []\n",
    "        train_kl_loss = []\n",
    "        for batch in train_iter:\n",
    "            x = batch.text\n",
    "            G_inp = create_generator_input(x, train=True)\n",
    "            rec_loss, kl_loss = train_batch(x, G_inp, step, train=True)\n",
    "            train_rec_loss.append(rec_loss)\n",
    "            train_kl_loss.append(kl_loss)\n",
    "            step += 1\n",
    "\n",
    "        vae.eval()\n",
    "        valid_rec_loss = []\n",
    "        valid_kl_loss = []\n",
    "        for batch in val_iter:\n",
    "            x = batch.text\n",
    "            G_inp = create_generator_input(x, train=False)\n",
    "            with torch.no_grad():\n",
    "                rec_loss, kl_loss = train_batch(x, G_inp, step, train=False)\n",
    "            valid_rec_loss.append(rec_loss)\n",
    "            valid_kl_loss.append(kl_loss)\n",
    "\n",
    "        train_rec_loss = np.mean(train_rec_loss)\n",
    "        train_kl_loss = np.mean(train_kl_loss)\n",
    "        valid_rec_loss = np.mean(valid_rec_loss)\n",
    "        valid_kl_loss = np.mean(valid_kl_loss)\n",
    "\n",
    "        print(\"No.\", epoch, \"T_rec:\", '%.2f' % train_rec_loss, \"T_kld:\", '%.2f' % train_kl_loss, \"V_rec:\", '%.2f' % valid_rec_loss, \"V_kld:\", '%.2f' % valid_kl_loss)\n",
    "        if epoch % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'vae_dict': vae.state_dict(),\n",
    "                'vae_trainer': trainer_vae.state_dict(),\n",
    "                'step': step\n",
    "            }, save_path)\n",
    "\n",
    "def generate_sentences(n_examples):\n",
    "    checkpoint = torch.load(save_path, map_location='cpu')\n",
    "    vae.load_state_dict(checkpoint['vae_dict'])\n",
    "    vae.eval()\n",
    "    del checkpoint\n",
    "    for i in range(n_examples):\n",
    "        z = torch.randn([1, opt.n_z])\n",
    "        h_0 = torch.zeros(opt.n_layers_G, 1, opt.n_hidden_G)\n",
    "        c_0 = torch.zeros(opt.n_layers_G, 1, opt.n_hidden_G)\n",
    "        G_hidden = (h_0, c_0)\n",
    "        G_inp = torch.LongTensor(1, 1).fill_(vocab.stoi[opt.start_token])\n",
    "        G_inp = get_cuda(G_inp)\n",
    "        str = opt.start_token + \" \"\n",
    "        while G_inp[0][0].item() != vocab.stoi[opt.end_token]:\n",
    "            with torch.no_grad():\n",
    "                logit, G_hidden, _ = vae(None, G_inp, z, G_hidden)\n",
    "            probs = F.softmax(logit[0], dim=1)\n",
    "            G_inp = torch.multinomial(probs, 1)\n",
    "            str += (vocab.itos[G_inp[0][0].item()] + \" \")\n",
    "        print(str.encode('utf-8'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if opt.to_train:\n",
    "        training()\n",
    "    else:\n",
    "        generate_sentences(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f97ba7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b11bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
